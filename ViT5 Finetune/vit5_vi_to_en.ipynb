{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e62661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion\\anaconda3\\envs\\nlp_proj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3a8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTokenizedDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Hàm chuẩn bị dữ liệu (tokenize toàn bộ trước)\n",
    "def prepare_data(vi_file, en_file, tokenizer, max_length=256):\n",
    "    with open(vi_file, 'r', encoding='utf-8') as f:\n",
    "        vi_texts = [line.strip() for line in f]\n",
    "    with open(en_file, 'r', encoding='utf-8') as f:\n",
    "        en_texts = [line.strip() for line in f]\n",
    "\n",
    "    assert len(vi_texts) == len(en_texts), \"Số lượng câu không khớp!\"\n",
    "\n",
    "    src_texts = [f\"vi-en: {vi}\" for vi in vi_texts]\n",
    "    \n",
    "    src_enc = tokenizer(\n",
    "        src_texts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tgt_enc = tokenizer(\n",
    "        en_texts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = tgt_enc[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return PreTokenizedDataset(src_enc[\"input_ids\"], src_enc[\"attention_mask\"], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791f304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Di chuyển data lên GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa813edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(model, dataloader, tokenizer, device, max_samples=None):\n",
    "    \"\"\"Tính BLEU score cho model trên dataset bằng SacreBLEU\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Evaluating BLEU\")):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "                \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Tính loss\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # Generate predictions\n",
    "            generated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=512,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "            # Decode predictions và references\n",
    "            for j in range(len(generated)):\n",
    "                # Decode prediction\n",
    "                pred = tokenizer.decode(generated[j], skip_special_tokens=True)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "                # Decode reference (target)\n",
    "                label = labels[j].cpu().numpy()\n",
    "                label[label == -100] = tokenizer.pad_token_id\n",
    "                ref = tokenizer.decode(label, skip_special_tokens=True)\n",
    "                references.append(ref)\n",
    "    \n",
    "    # Tính BLEU score bằng SacreBLEU\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "    bleu_score = bleu.score \n",
    "    \n",
    "    avg_loss = total_loss / min(len(dataloader), max_samples or len(dataloader))\n",
    "    \n",
    "    return avg_loss, bleu_score, predictions[:5], references[:5]  # Return 5 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838f884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 62500\n",
      "Training samples: 500000\n",
      "Test samples: 3000\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 62500/62500 [4:37:42<00:00,  3.75it/s, loss=0.978]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.4559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU:  27%|██▋       | 100/375 [02:24<06:36,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8961\n",
      "BLEU Score: 37.4292 (3742.9)\n",
      "\n",
      "Sample translations:\n",
      "Pred: Knowledge and practice of people with health insurance cards in using medical examination and treatment services in public health facilities and some influencing factors in Viêng Chăn, Lao, 2017\n",
      "Ref:  Knowledge, practices in public health service utilization among health insurance cards holders and influencing factors in Vientiane, Lao\n",
      "\n",
      "Pred: To describe the current situation of knowledge and practice of people with health insurance cards in the use of medical examination and treatment services in public health facilities and some related factors in Viêng Chăn Province, Lao People's Democratic Republic of Lao in 2017.\n",
      "Ref:  Describe knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao PDR, 2017.\n",
      "\n",
      "Pred: Methods: A cross-sectional descriptive study was conducted on 928 adults with health insurance cards in Phone Hong and Keo Oudom districts, Viêng Chăn province.\n",
      "Ref:  Methodology: A cross sectional study was used among 928 adult health insurance card's holders in Phone Hong and Keo Oudom districts, Vientiane province.\n",
      "\n",
      "New best BLEU score! Saving model...\n",
      "\n",
      "Training completed!\n",
      "Best BLEU Score: 37.4292\n",
      "Model saved to: ./vit5_finetuned_vi_to_en\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"VietAI/vit5-base\"  \n",
    "BATCH_SIZE = 8      \n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 1\n",
    "MAX_LENGTH = 128\n",
    "OUTPUT_DIR = \"./vit5_finetuned_vi_to_en\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer và model\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "# Tạo datasets\n",
    "train_dataset = prepare_data(\"Released Corpus/train.vi.txt\", \"Released Corpus/train.en.txt\", tokenizer, MAX_LENGTH)\n",
    "test_dataset = prepare_data(\"Released Corpus/test.vi.txt\", \"Released Corpus/test.en.txt\", tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Tạo dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Optimizer và scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "best_bleu = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_loss, bleu_score, sample_preds, sample_refs = evaluate_bleu(\n",
    "        model, test_dataloader, tokenizer, device, max_samples=100\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f} ({bleu_score*100:.1f})\")  \n",
    "    \n",
    "    print(\"\\nSample translations:\")\n",
    "    for i in range(min(3, len(sample_preds))):\n",
    "        print(f\"Pred: {sample_preds[i]}\")\n",
    "        print(f\"Ref:  {sample_refs[i]}\")\n",
    "        print()\n",
    "    \n",
    "    if bleu_score > best_bleu:\n",
    "        best_bleu = bleu_score\n",
    "        best_val_loss = val_loss\n",
    "        print(\"New best BLEU score! Saving model...\")\n",
    "        \n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.makedirs(OUTPUT_DIR)\n",
    "            \n",
    "        model.save_pretrained(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        \n",
    "        with open(os.path.join(OUTPUT_DIR, \"training_info.txt\"), \"w\") as f:\n",
    "            f.write(f\"Best BLEU Score: {best_bleu:.4f}\\n\")\n",
    "            f.write(f\"Best Epoch: {epoch + 1}\\n\")\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best BLEU Score: {best_bleu:.4f}\")\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca493c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test_evaluation(model_path, test_vi_file, test_en_file):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"FINAL EVALUATION ON FULL TEST SET\")\n",
    "    \n",
    "    # Load model và tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test data\n",
    "    with open(test_en_file, 'r', encoding='utf-8') as f:\n",
    "        test_en_texts = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    with open(test_vi_file, 'r', encoding='utf-8') as f:\n",
    "        test_vi_texts = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    predictions = []\n",
    "    references = test_en_texts\n",
    "    \n",
    "    print(f\"Translating {len(test_vi_texts)} test sentences...\")\n",
    "    \n",
    "    for i, vi_text in enumerate(tqdm(test_vi_texts, desc=\"Translating\")):\n",
    "        # Thêm prefix\n",
    "        input_text = f\"vi-en: {vi_text}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors='pt',\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=512,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(translation)\n",
    "    \n",
    "    # Tính BLEU score bằng SacreBLEU\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "    \n",
    "    print(\"FINAL TEST RESULTS:\")\n",
    "    print(f\"Total test samples: {len(test_vi_texts)}\")\n",
    "    print(f\"BLEU Score: {bleu.score:.2f}\")\n",
    "    print(f\"BLEU Details: {bleu}\")\n",
    "    \n",
    "    # Save kết quả\n",
    "    results_file = os.path.join(model_path, \"final_test_results.txt\")\n",
    "    with open(results_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(f\"Final Test Results\\n\")\n",
    "        f.write(f\"Total test samples: {len(test_en_texts)}\\n\")\n",
    "        f.write(f\"BLEU Score: {bleu.score:.2f}\\n\")\n",
    "        f.write(f\"BLEU Details: {bleu}\\n\")\n",
    "        for i in range(min(10, len(predictions))):\n",
    "            f.write(f\"\\nExample {i+1}:\\n\")\n",
    "            f.write(f\"VI: {test_vi_texts[i]}\\n\")\n",
    "            f.write(f\"Pred: {predictions[i]}\\n\")\n",
    "            f.write(f\"Ref: {references[i]}\\n\")\n",
    "    \n",
    "    print(f\"Detailed results saved to: {results_file}\")\n",
    "    \n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c9da7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL EVALUATION ON FULL TEST SET\n",
      "Translating 3000 test sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 3000/3000 [30:57<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TEST RESULTS:\n",
      "Total test samples: 3000\n",
      "BLEU Score: 34.17\n",
      "BLEU Details: BLEU = 34.17 65.8/41.5/28.7/20.8 (BP = 0.957 ratio = 0.958 hyp_len = 73367 ref_len = 76604)\n",
      "Detailed results saved to: ./vit5_finetuned_vi_to_en\\final_test_results.txt\n",
      "\n",
      "Final BLEU-4 score on full test set: 34.1713\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation trên toàn bộ test set\n",
    "if os.path.exists(\"./vit5_finetuned_vi_to_en\"):\n",
    "    final_bleu = final_test_evaluation(\"./vit5_finetuned_vi_to_en\", \"Released Corpus/test.vi.txt\", \"Released Corpus/test.en.txt\")\n",
    "    print(f\"\\nFinal BLEU-4 score on full test set: {final_bleu:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196bdaa8",
   "metadata": {},
   "source": [
    "## Import libraries and model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e549a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220e6f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "D_MODEL = 256 \n",
    "NHEAD = 8\n",
    "NUM_ENCODER_LAYERS = 4 \n",
    "NUM_DECODER_LAYERS = 4 \n",
    "DIM_FEEDFORWARD = 1024\n",
    "DROPOUT = 0.1 \n",
    "\n",
    "BATCHSIZE = 16\n",
    "LR = 5e-5\n",
    "EPOCH = 10\n",
    "WARMUP_STEPS = 1000\n",
    "\n",
    "VOCAB_DIR = \"saved_vocab\"\n",
    "MODELNAME = \"NMT_transformer.model\"\n",
    "FINETUNEMODELNAME = \"NMT_transformer_finetune.model\"\n",
    "\n",
    "TRAIN_EN_PATH = \"./Released Corpus/train.en.txt\"\n",
    "TRAIN_VI_PATH = \"./Released Corpus/train.vi.txt\"\n",
    "TEST_EN_PATH  = \"./Released Corpus/test.en.txt\"\n",
    "TEST_VI_PATH  = \"./Released Corpus/test.vi.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5251571",
   "metadata": {},
   "source": [
    "## Positional encoding và Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d106a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2], pe[:, 1::2] = torch.sin(position * div_term), torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:x.size(0)])\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1, ignore_index=None):\n",
    "        super().__init__()\n",
    "        self.confidence, self.smoothing, self.cls, self.dim, self.ignore_index = 1.0 - smoothing, smoothing, classes, dim, ignore_index\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        if self.smoothing == 0:\n",
    "            return F.nll_loss(pred, target, ignore_index=self.ignore_index)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.full_like(pred, self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "            if self.ignore_index is not None:\n",
    "                true_dist.masked_fill_(target.eq(self.ignore_index).unsqueeze(1), 0)\n",
    "        loss = torch.sum(-true_dist * pred, dim=self.dim)\n",
    "        if self.ignore_index is not None:\n",
    "            non_pad = (~target.eq(self.ignore_index)).sum()\n",
    "            return loss.sum() / non_pad\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c4207",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16783237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4, \n",
    "                 dim_feedforward=1024, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model, padding_idx=1)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model, padding_idx=1)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation='gelu', norm_first=True)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation='gelu', norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_encoder_layers)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_decoder_layers)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src = self.pos_encoder(self.src_embed(src) * math.sqrt(self.d_model))\n",
    "        tgt = self.pos_encoder(self.tgt_embed(tgt) * math.sqrt(self.d_model))\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n",
    "        return self.fc_out(self.layer_norm(output))\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz), diagonal=1).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2047b9",
   "metadata": {},
   "source": [
    "## Warmup Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcdb7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self._step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        lr = self._get_lr()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _get_lr(self):\n",
    "        return (self.d_model ** -0.5) * min(\n",
    "            self._step ** -0.5,\n",
    "            self._step * (self.warmup_steps ** -1.5)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f809b40",
   "metadata": {},
   "source": [
    "## Load vocab và tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19a7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_dir, file_name_en, file_name_vi):\n",
    "    with open(os.path.join(vocab_dir, file_name_en), \"rb\") as f:\n",
    "        vocab_en_data = pickle.load(f)\n",
    "        vocablist_en, vocabidx_en = vocab_en_data[\"list\"], vocab_en_data[\"idx\"]\n",
    "    with open(os.path.join(vocab_dir, file_name_vi), \"rb\") as f:\n",
    "        vocab_vi_data = pickle.load(f)\n",
    "        vocablist_vi, vocabidx_vi = vocab_vi_data[\"list\"], vocab_vi_data[\"idx\"]\n",
    "    return vocablist_en, vocabidx_en, vocablist_vi, vocabidx_vi\n",
    "\n",
    "def save_vocab(vocablist_en, vocabidx_en, vocablist_vi, vocabidx_vi, vocab_dir):\n",
    "    os.makedirs(vocab_dir, exist_ok=True)\n",
    "    with open(os.path.join(vocab_dir, \"vocab_en_new.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\"list\": vocablist_en, \"idx\": vocabidx_en}, f)\n",
    "    with open(os.path.join(vocab_dir, \"vocab_vi_new.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\"list\": vocablist_vi, \"idx\": vocabidx_vi}, f)\n",
    "    print(\"Saved updated vocab to\", vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26d20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip().split() for line in f]\n",
    "\n",
    "def collect_new_tokens_topk(train_en_new, train_vi_new, test_en_new, test_vi_new,\n",
    "                            vocabidx_en, vocabidx_vi, top_k_en=5000, top_k_vi=5000):\n",
    "    def count_tokens(data_lists):\n",
    "        counter = Counter()\n",
    "        for data in data_lists:\n",
    "            for sent in data:\n",
    "                counter.update(sent)\n",
    "        return counter\n",
    "\n",
    "    freq_en = count_tokens([train_en_new, test_en_new])\n",
    "    freq_vi = count_tokens([train_vi_new, test_vi_new])\n",
    "\n",
    "    new_en_tokens = [token for token in freq_en if token not in vocabidx_en]\n",
    "    new_vi_tokens = [token for token in freq_vi if token not in vocabidx_vi]\n",
    "\n",
    "    new_en_tokens_topk = sorted(new_en_tokens, key=lambda t: freq_en[t], reverse=True)[:top_k_en]\n",
    "    new_vi_tokens_topk = sorted(new_vi_tokens, key=lambda t: freq_vi[t], reverse=True)[:top_k_vi]\n",
    "\n",
    "    print(f\"Selected top-{top_k_en} new English tokens:\", len(new_en_tokens_topk))\n",
    "    print(f\"Selected top-{top_k_vi} new Vietnamese tokens:\", len(new_vi_tokens_topk))\n",
    "    return new_en_tokens_topk, new_vi_tokens_topk\n",
    "\n",
    "def preprocess_with_special_tokens(data, vocabidx):\n",
    "    rr = []\n",
    "    for tokenlist in data:\n",
    "        tkl = ['<cls>']\n",
    "        for token in tokenlist:\n",
    "            tkl.append(token if token in vocabidx else '<unk>')\n",
    "        tkl.append('<eos>')\n",
    "        rr.append(tkl)\n",
    "    return rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3257930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_train_data(train_data):\n",
    "    batched = []\n",
    "    for i in range(0, len(train_data), BATCHSIZE):\n",
    "        batch = train_data[i:i+BATCHSIZE]\n",
    "        en_batch = [x[0] for x in batch]\n",
    "        vi_batch = [x[1] for x in batch]\n",
    "\n",
    "        maxlen_en = max(len(seq) for seq in en_batch)\n",
    "        maxlen_vi = max(len(seq) for seq in vi_batch)\n",
    "\n",
    "        en_batch = [seq + [vocabidx_en['<pad>']]*(maxlen_en - len(seq)) for seq in en_batch]\n",
    "        vi_batch = [seq + [vocabidx_vi['<pad>']]*(maxlen_vi - len(seq)) for seq in vi_batch]\n",
    "\n",
    "        batched.append((en_batch, vi_batch))\n",
    "    return batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd6e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_new_embeddings_with_avg(old_embedding, new_vocab_size):\n",
    "    old_num_embeddings, embedding_dim = old_embedding.weight.size()\n",
    "    device = old_embedding.weight.device  \n",
    "\n",
    "    new_embedding = nn.Embedding(new_vocab_size, embedding_dim).to(device)\n",
    "    new_embedding.weight.data[:old_num_embeddings] = old_embedding.weight.data\n",
    "\n",
    "    # Tính mean embedding của các token cũ (trừ <pad>)\n",
    "    mask = torch.arange(old_num_embeddings, device=device) != 0\n",
    "    mean_vec = old_embedding.weight.data[mask].mean(dim=0)\n",
    "\n",
    "    # Khởi tạo token mới bằng mean_vec + noise\n",
    "    for i in range(old_num_embeddings, new_vocab_size):\n",
    "        new_embedding.weight.data[i] = mean_vec + torch.randn(embedding_dim, device=device) * (embedding_dim ** -0.5)\n",
    "\n",
    "    return new_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e22a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_output_layer_smart(old_fc, new_vocab_size, d_model):\n",
    "    old_out_features, old_in_features = old_fc.weight.size()\n",
    "    device = old_fc.weight.device  \n",
    "\n",
    "    new_fc = nn.Linear(d_model, new_vocab_size).to(device)\n",
    "    new_fc.weight.data[:old_out_features] = old_fc.weight.data\n",
    "    new_fc.bias.data[:old_out_features] = old_fc.bias.data\n",
    "\n",
    "    mean_w = old_fc.weight.data.mean(dim=0)\n",
    "    for i in range(old_out_features, new_vocab_size):\n",
    "        new_fc.weight.data[i] = mean_w + torch.randn(d_model, device=device) * (d_model ** -0.5)\n",
    "        new_fc.bias.data[i] = 0.0\n",
    "\n",
    "    return new_fc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bd9ef",
   "metadata": {},
   "source": [
    "## Hàm greedy decoding và đánh giá BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "360e5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, src_key_padding_mask,\n",
    "                  max_len=60, start_symbol=None, end_symbol=None, device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        src_emb = model.pos_encoder(model.src_embed(src) * math.sqrt(model.d_model))\n",
    "        memory = model.encoder(src_emb, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        ys = torch.tensor([[start_symbol]], dtype=torch.long, device=device) \n",
    "        for i in range(max_len - 1):\n",
    "            tgt_emb = model.pos_encoder(model.tgt_embed(ys) * math.sqrt(model.d_model))\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt_emb.size(0)).to(device)\n",
    "            out = model.decoder(tgt_emb, memory, tgt_mask=tgt_mask,\n",
    "                                tgt_key_padding_mask=None,\n",
    "                                memory_key_padding_mask=src_key_padding_mask)\n",
    "            out = model.fc_out(model.layer_norm(out)) \n",
    "            prob = F.log_softmax(out[-1, 0], dim=-1)  \n",
    "            next_token = torch.argmax(prob).unsqueeze(0).unsqueeze(0)  \n",
    "            ys = torch.cat([ys, next_token], dim=0)\n",
    "            if next_token.item() == end_symbol:\n",
    "                break\n",
    "\n",
    "    return ys.squeeze(1).tolist() \n",
    "\n",
    "\n",
    "def evaluate_bleu_greedy(model, test_data, vocabidx_en, vocabidx_vi,\n",
    "                         max_len=60, device=DEVICE, print_samples=False, num_samples=5):\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    idx2word_vi = {v: k for k, v in vocabidx_vi.items()}\n",
    "    idx2word_en = {v: k for k, v in vocabidx_en.items()}\n",
    "\n",
    "    start_symbol = vocabidx_vi.get('<sos>', vocabidx_vi.get('<cls>', None))\n",
    "    end_symbol = vocabidx_vi.get('<eos>', vocabidx_vi.get('</s>', vocabidx_vi.get('<eos>', None)))\n",
    "    pad_vi = vocabidx_vi.get('<pad>', None)\n",
    "    pad_en = vocabidx_en.get('<pad>', None)\n",
    "\n",
    "    assert start_symbol is not None and end_symbol is not None, \"Start or end token not found in target vocab.\"\n",
    "\n",
    "    sample_outputs = []\n",
    "\n",
    "    for idx, item in enumerate(test_data):\n",
    "        if len(item) == 2:\n",
    "            en_idx, vi_idx = item\n",
    "            vi_orig_tokens = [idx2word_vi[t] for t in vi_idx if t not in (start_symbol, end_symbol, pad_vi)]\n",
    "            en_orig_tokens = [idx2word_en[t] for t in en_idx if t != pad_en]\n",
    "        else:\n",
    "            en_idx, en_orig_tokens, vi_orig_tokens = item\n",
    "\n",
    "        src = torch.tensor(en_idx, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        src_mask = torch.zeros((src.size(0), src.size(0)), device=device).bool()\n",
    "        src_key_padding_mask = (src.transpose(0, 1) == pad_en)\n",
    "\n",
    "        out_tokens = greedy_decode(model, src, src_mask, src_key_padding_mask,\n",
    "                                   max_len=max_len, start_symbol=start_symbol, end_symbol=end_symbol, device=device)\n",
    "        hyp_tokens = [idx2word_vi[t] for t in out_tokens if t not in (start_symbol, end_symbol, pad_vi)]\n",
    "\n",
    "        refs.append([vi_orig_tokens])\n",
    "        hyps.append(hyp_tokens)\n",
    "\n",
    "        if print_samples and idx < num_samples:\n",
    "            sample_outputs.append((\n",
    "                \" \".join(en_orig_tokens),\n",
    "                \" \".join(hyp_tokens),\n",
    "                \" \".join(vi_orig_tokens)\n",
    "            ))\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(refs, hyps, smoothing_function=smoothie)\n",
    "\n",
    "    if print_samples:\n",
    "        print(\"\\n--- Sample Translations ---\")\n",
    "        for i, (src_sent, hyp_sent, ref_sent) in enumerate(sample_outputs, 1):\n",
    "            print(f\"[{i}] EN : {src_sent}\")\n",
    "            print(f\"    PRED: {hyp_sent}\")\n",
    "            print(f\"    REF : {ref_sent}\")\n",
    "            print()\n",
    "\n",
    "    return bleu_score, [\" \".join(h) for h in hyps]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f8cb6",
   "metadata": {},
   "source": [
    "## Hàm finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d3fb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, train_batches, test_data, optimizer, scheduler, criterion,\n",
    "             vocabidx_en, vocabidx_vi,\n",
    "             epochs, save_path,\n",
    "             freeze_epochs, max_decoding_len):\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    best_bleu = 0.0\n",
    "    init_lr = optimizer.defaults.get('lr', 5e-5)\n",
    "\n",
    "    # Freeze all except embeddings, fc_out, layernorm\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.src_embed.weight.requires_grad = True\n",
    "    model.tgt_embed.weight.requires_grad = True\n",
    "    model.fc_out.weight.requires_grad = True\n",
    "    model.fc_out.bias.requires_grad = True\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"layer_norm\" in name or \"layernorm\" in name.lower():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # Rebuild optimizer for trainable params\n",
    "    optimizer = optim.Adam((p for p in model.parameters() if p.requires_grad),\n",
    "                           lr=init_lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = WarmupScheduler(optimizer, model.d_model,\n",
    "                                getattr(scheduler, 'warmup_steps', WARMUP_STEPS))\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for en_batch, vi_batch in tqdm(train_batches, desc=f\"Epoch {epoch}\"):\n",
    "            src = torch.tensor(en_batch, dtype=torch.long, device=device).transpose(0, 1)\n",
    "            tgt = torch.tensor(vi_batch, dtype=torch.long, device=device).transpose(0, 1)\n",
    "\n",
    "            tgt_input = tgt[:-1]\n",
    "            tgt_output = tgt[1:]\n",
    "\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n",
    "            src_key_padding_mask = (src.transpose(0, 1) == vocabidx_en['<pad>'])\n",
    "            tgt_key_padding_mask = (tgt_input.transpose(0, 1) == vocabidx_vi['<pad>'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input, tgt_mask=tgt_mask,\n",
    "                           src_key_padding_mask=src_key_padding_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch} - Loss: {total_loss/len(train_batches):.4f}\")\n",
    "\n",
    "        # Unfreeze all after freeze_epochs\n",
    "        if epoch == freeze_epochs:\n",
    "            print(\"=> Unfreezing all parameters\")\n",
    "            for p in model.parameters():\n",
    "                p.requires_grad = True\n",
    "            optimizer = optim.Adam(model.parameters(), lr=max(1e-6, init_lr/2),\n",
    "                                   betas=(0.9, 0.98), eps=1e-9)\n",
    "            scheduler = WarmupScheduler(optimizer, model.d_model,\n",
    "                                        max(200, getattr(scheduler, 'warmup_steps', WARMUP_STEPS)//2))\n",
    "\n",
    "        # Evaluate BLEU \n",
    "        bleu, _ = evaluate_bleu_greedy(\n",
    "            model, test_data, vocabidx_en, vocabidx_vi,\n",
    "            max_len=max_decoding_len, device=device\n",
    "        )\n",
    "        print(f\"BLEU: {bleu*100:.2f}\")\n",
    "\n",
    "        if bleu > best_bleu:\n",
    "            best_bleu = bleu\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"config\": {\n",
    "                    \"src_vocab_size\": len(vocabidx_en),\n",
    "                    \"tgt_vocab_size\": len(vocabidx_vi),\n",
    "                    \"d_model\": model.d_model,\n",
    "                    \"nhead\": NHEAD,\n",
    "                    \"num_encoder_layers\": NUM_ENCODER_LAYERS,\n",
    "                    \"num_decoder_layers\": NUM_DECODER_LAYERS,\n",
    "                    \"dim_feedforward\": DIM_FEEDFORWARD,\n",
    "                    \"dropout\": DROPOUT\n",
    "                }\n",
    "            }, save_path)\n",
    "            print(f\"Saved best model with BLEU={best_bleu*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b739a",
   "metadata": {},
   "source": [
    "## Thực hiện load data, vocab và xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd72430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab sizes: EN = 24420 VI = 10666\n",
      "Selected top-5000 new English tokens: 5000\n",
      "Selected top-5000 new Vietnamese tokens: 5000\n",
      "After expansion: EN = 29420 VI = 15666\n",
      "Saved updated vocab to saved_vocab\n",
      "Loaded 500000 training pairs\n",
      "Loaded 3000 test pairs\n",
      "Padded into 31250 batches\n"
     ]
    }
   ],
   "source": [
    "vocablist_en, vocabidx_en, vocablist_vi, vocabidx_vi = load_vocab(\n",
    "    VOCAB_DIR, \"vocab_en.pkl\", \"vocab_vi.pkl\"\n",
    ")\n",
    "\n",
    "print(\"Loaded vocab sizes: EN =\", len(vocablist_en), \"VI =\", len(vocablist_vi))\n",
    "\n",
    "# load released corpus\n",
    "train_en_new = load_file(TRAIN_EN_PATH)\n",
    "train_vi_new = load_file(TRAIN_VI_PATH)\n",
    "test_en_new  = load_file(TEST_EN_PATH)\n",
    "test_vi_new  = load_file(TEST_VI_PATH)\n",
    "\n",
    "# collect new tokens top-k\n",
    "new_en_tokens, new_vi_tokens = collect_new_tokens_topk(\n",
    "    train_en_new, train_vi_new, test_en_new, test_vi_new,\n",
    "    vocabidx_en, vocabidx_vi,\n",
    "    top_k_en=5000, top_k_vi=5000\n",
    ")\n",
    "\n",
    "for token in new_en_tokens:\n",
    "    vocabidx_en[token] = len(vocablist_en)\n",
    "    vocablist_en.append(token)\n",
    "for token in new_vi_tokens:\n",
    "    vocabidx_vi[token] = len(vocablist_vi)\n",
    "    vocablist_vi.append(token)\n",
    "\n",
    "print(\"After expansion: EN =\", len(vocablist_en), \"VI =\", len(vocablist_vi))\n",
    "save_vocab(vocablist_en, vocabidx_en, vocablist_vi, vocabidx_vi, VOCAB_DIR)\n",
    "\n",
    "# preprocess to tokens with special tokens\n",
    "train_en_prep_new = preprocess_with_special_tokens(train_en_new, vocabidx_en)\n",
    "train_vi_prep_new = preprocess_with_special_tokens(train_vi_new, vocabidx_vi)\n",
    "test_en_prep_new  = preprocess_with_special_tokens(test_en_new, vocabidx_en)\n",
    "test_vi_prep_new  = preprocess_with_special_tokens(test_vi_new, vocabidx_vi)\n",
    "\n",
    "# convert to indices \n",
    "train_data_new = [\n",
    "    (\n",
    "        [vocabidx_en[token] for token in en_tokens],\n",
    "        [vocabidx_vi[token] for token in vi_tokens]\n",
    "    )\n",
    "    for en_tokens, vi_tokens in zip(train_en_prep_new, train_vi_prep_new)\n",
    "]\n",
    "print(\"Loaded\", len(train_data_new), \"training pairs\")\n",
    "\n",
    "# convert to indices and keep original tokens for reference\n",
    "test_data_new = [\n",
    "    (\n",
    "        [vocabidx_en[token] for token in en_tokens],\n",
    "        en_original,\n",
    "        vi_original\n",
    "    )\n",
    "    for en_tokens, en_original, vi_original in zip(test_en_prep_new, test_en_new, test_vi_new)\n",
    "]\n",
    "print(\"Loaded\", len(test_data_new), \"test pairs\")\n",
    "\n",
    "train_batches_new = padding_train_data(train_data_new)\n",
    "print(\"Padded into\", len(train_batches_new), \"batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116af53e",
   "metadata": {},
   "source": [
    "## Load model đã pretrain và finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec742043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion\\anaconda3\\envs\\nlp_proj\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model (strict=False)\n",
      "Expanding source embedding: 24420 -> 29420\n",
      "Expanding target embedding: 10666 -> 15666\n",
      "Expanding fc_out: 10666 -> 15666\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(MODELNAME, map_location=DEVICE)\n",
    "config = checkpoint.get(\"config\", None)\n",
    "\n",
    "model = Transformer(config[\"src_vocab_size\"], config[\"tgt_vocab_size\"], config[\"d_model\"], config[\"nhead\"],\n",
    "                    config[\"num_encoder_layers\"], config[\"num_decoder_layers\"],\n",
    "                    config[\"dim_feedforward\"], config[\"dropout\"]).to(DEVICE)\n",
    "\n",
    "state = checkpoint.get(\"model_state\", checkpoint)\n",
    "model.load_state_dict(state, strict=False)\n",
    "print(\"Loaded pretrained model (strict=False)\")\n",
    "\n",
    "# expand embeddings, output layer if needed using smart init\n",
    "old_src_vocab_size = model.src_embed.num_embeddings\n",
    "old_tgt_vocab_size = model.tgt_embed.num_embeddings\n",
    "\n",
    "if len(vocablist_en) > old_src_vocab_size:\n",
    "    print(f\"Expanding source embedding: {old_src_vocab_size} -> {len(vocablist_en)}\")\n",
    "    model.src_embed = init_new_embeddings_with_avg(model.src_embed, len(vocablist_en)).to(DEVICE)\n",
    "\n",
    "if len(vocablist_vi) > old_tgt_vocab_size:\n",
    "    print(f\"Expanding target embedding: {old_tgt_vocab_size} -> {len(vocablist_vi)}\")\n",
    "    model.tgt_embed = init_new_embeddings_with_avg(model.tgt_embed, len(vocablist_vi)).to(DEVICE)\n",
    "\n",
    "# expand output layer smartly\n",
    "old_fc_out_weight = model.fc_out.weight.data\n",
    "old_out_features, embedding_dim = old_fc_out_weight.size()\n",
    "if len(vocablist_vi) > old_out_features:\n",
    "    print(f\"Expanding fc_out: {old_out_features} -> {len(vocablist_vi)}\")\n",
    "    model.fc_out = expand_output_layer_smart(model.fc_out, len(vocablist_vi), D_MODEL).to(DEVICE)\n",
    "\n",
    "# optimizer, scheduler, criterion for finetune\n",
    "FINETUNE_LR = 5e-5\n",
    "FINETUNE_WARMUP = 1000\n",
    "FREEZE_EPOCHS = 5\n",
    "MAX_DECODE_LEN = 60\n",
    "\n",
    "# create a dummy optimizer/scheduler to pass structure to finetune\n",
    "_dummy_optimizer = optim.Adam(model.parameters(), lr=FINETUNE_LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "_dummy_scheduler = WarmupScheduler(_dummy_optimizer, D_MODEL, FINETUNE_WARMUP)\n",
    "criterion = LabelSmoothingLoss(len(vocablist_vi), smoothing=0.05, ignore_index=vocabidx_vi['<pad>'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e671473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 31250/31250 [15:59<00:00, 32.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 3.4822\n",
      "BLEU: 23.60\n",
      "Saved best model with BLEU=23.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 31250/31250 [14:41<00:00, 35.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 3.1838\n",
      "BLEU: 24.95\n",
      "Saved best model with BLEU=24.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 31250/31250 [14:48<00:00, 35.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 3.1132\n",
      "BLEU: 25.36\n",
      "Saved best model with BLEU=25.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 31250/31250 [14:43<00:00, 35.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 3.0765\n",
      "BLEU: 25.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 31250/31250 [14:44<00:00, 35.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 3.0543\n",
      "=> Unfreezing all parameters\n",
      "BLEU: 25.59\n",
      "Saved best model with BLEU=25.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 31250/31250 [18:04<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 3.4589\n",
      "BLEU: 22.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 31250/31250 [18:04<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 3.0875\n",
      "BLEU: 24.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 31250/31250 [18:05<00:00, 28.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 2.9816\n",
      "BLEU: 25.88\n",
      "Saved best model with BLEU=25.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 31250/31250 [18:07<00:00, 28.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Loss: 2.9190\n",
      "BLEU: 26.66\n",
      "Saved best model with BLEU=26.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 31250/31250 [18:04<00:00, 28.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Loss: 2.8834\n",
      "BLEU: 26.91\n",
      "Saved best model with BLEU=26.91\n"
     ]
    }
   ],
   "source": [
    "finetune(model, train_batches_new, test_data_new, _dummy_optimizer, _dummy_scheduler, criterion,\n",
    "             vocabidx_en, vocabidx_vi,\n",
    "             EPOCH, FINETUNEMODELNAME,\n",
    "             FREEZE_EPOCHS, MAX_DECODE_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03351ab",
   "metadata": {},
   "source": [
    "## Kết quả cuối cùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7122278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Translations ---\n",
      "[1] EN : Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao\n",
      "    PRED: Kiến thức, thực hành về sử dụng dịch vụ y tế công cộng của người dân có tham gia bảo hiểm y tế và các yếu tố ảnh hưởng đến kiến thức, thực hành của người dân tại tỉnh dân <unk>\n",
      "    REF : Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017\n",
      "\n",
      "[2] EN : Describe knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao PDR, 2017.\n",
      "    PRED: Mô tả kiến thức, thực hành về sử dụng dịch vụ y tế công cộng của người dân có tham gia bảo hiểm y tế và các yếu tố ảnh hưởng đến kiến thức, thực hành của người dân tại tỉnh <unk> năm 2017.\n",
      "    REF : Mô tả thực trạng kiến thức, thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố liên quan tại tỉnh Viêng Chăn, Cộng hoà Dân chủ Nhân dân Lào năm 2017.\n",
      "\n",
      "[3] EN : Methodology: A cross sectional study was used among 928 adult health insurance card's holders in Phone Hong and Keo Oudom districts, Vientiane province.\n",
      "    PRED: Phương pháp nghiên cứu: Nghiên cứu mô tả cắt ngang được sử dụng trên <unk> người trưởng thành có tham gia bảo hiểm y tế ở người trưởng thành có tham gia vào buồng tim và người trưởng thành tại tỉnh <unk>\n",
      "    REF : Phương pháp: Thiết kế nghiên mô tả cắt ngang được thực hiện trên 928 người trưởng thành có thẻ bảo hiểm y tế tại 2 huyện Phone Hong và Keo Oudom, tỉnh Viêng Chăn.\n",
      "\n",
      "[4] EN : Results: Percentage of card's holders who knew the finance-free utilization of the first registered public health services was 44.5% and being provided health insurance information was 34.8%.\n",
      "    PRED: Kết quả: Tỷ lệ người dân có kiến thức về sử dụng dịch vụ y tế công cộng lần đầu là <unk> và đang được cung cấp thông tin bảo hiểm y tế là <unk>\n",
      "    REF : Kết quả: Tỷ lệ người biết được khám chữa bệnh (KCB) miễn phí tại nơi đăng ký ban đầu chiếm 44,5%, được cung cấp thông tin về bảo hiểm y tế (BHYT) chiếm 34,8%.\n",
      "\n",
      "[5] EN : Percentage of card's holders who went to the first registered public health services was 61.8%.\n",
      "    PRED: Tỷ lệ người có người dân có tham gia dịch vụ khám chữa bệnh lần đầu là <unk>\n",
      "    REF : Tỷ lệ người có thẻ BHYT thực hành khám chữa bệnh đúng nơi đăng ký KCB ban đầu chiếm 61,8%.\n",
      "\n",
      "[6] EN : Percentage of card's holders who went to public health services to receive medicines for their relatives / others people was 20.1%.\n",
      "    PRED: Tỷ lệ người có người có người có tham gia dịch vụ y tế công lập là <unk>\n",
      "    REF : Tỷ lệ người có thẻ BHYT sử dụng thẻ để lấy thuốc cho người khác khá cao (20,1%).\n",
      "\n",
      "[7] EN : The determinants of knowledge and practices in public health service utilization among health insurance card's holders were distance and time taken to health services, time of health insurance and health insurance information provided.\n",
      "    PRED: Các yếu tố liên quan đến kiến thức, thực hành về sử dụng dịch vụ y tế công cộng của người dân có tham gia bảo hiểm y tế là khoảng cách và thời gian thực hành khám chữa bệnh bảo hiểm y tế, thời gian bảo hiểm y tế và thông tin y tế.\n",
      "    REF : Các yếu tố khoảng cách từ nhà đến cơ sở y tế, thời gian tham gia BHYT và được tiếp nhận thông tin về BHYT của người có thẻ BHYT là những yếu tố ảnh hưởng đến kiến thức và thực hành sử dụng thẻ BHYT trong khám chữa bệnh.\n",
      "\n",
      "[8] EN : Conclusions: Knowledge and practices in public health service utilization among health insurance card's holders were still limited.\n",
      "    PRED: Kết luận: Kiến thức và thực hành về sử dụng dịch vụ y tế công cộng của người dân có tham gia bảo hiểm y tế còn hạn chế.\n",
      "    REF : Kết luận: Kiến thức và thực hành của người có thẻ BHYT trong sử dụng dịch vụ y tế công tại Cộng hoà Dân chủ Nhân dân Lào còn hạn chế.\n",
      "\n",
      "[9] EN : It's necessary to provide health insurance communication and education for people who live in remote areas and participate interupted health insurance.\n",
      "    PRED: Cần có sự tham gia bảo hiểm y tế và giáo dục cho người dân sống ở vùng xa và tham gia y tế cơ sở y tế.\n",
      "    REF : Cần tập trung vào truyền thông cho những nhóm người sống xa cơ sở y tế và những người tham gia bảo hiểm y tế không liên tục.\n",
      "\n",
      "[10] EN : Studying the method of quantification of diclofenac sodium in traditional herbal medicines used for treatment or support osteoarthritis by high performance liquid chromatography\n",
      "    PRED: Nghiên cứu phương pháp định lượng natri <unk> trong thuốc đông dược truyền thống được sử dụng để điều trị thoái hoá khớp bằng phương pháp sắc ký lỏng hiệu năng cao\n",
      "    REF : Nghiên cứu xác định thuốc diclofenac natri lẫn trong chế phẩm đông dược được sử dụng điều trị hoặc hỗ trợ điều trị các bệnh về xương khớp bằng phương pháp sắc ký lỏng hiệu năng cao\n",
      "\n",
      "BLEU: 26.91\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"NMT_transformer_finetune.model\")\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "bleu, _ = evaluate_bleu_greedy(\n",
    "    model, test_data_new, vocabidx_en, vocabidx_vi,\n",
    "    max_len=60, device=DEVICE, print_samples=True, num_samples=10\n",
    ")\n",
    "print(f\"BLEU: {bleu*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18c827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 1268 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "test_en  = load_file(\"./IWSLT/test.en\")\n",
    "test_vi  = load_file(\"./IWSLT/test.vi\")\n",
    "\n",
    "print(f\"Data loaded: {len(test_en)} test pairs\")\n",
    "\n",
    "test_en_prep  = preprocess_with_special_tokens(test_en, vocabidx_en)\n",
    "\n",
    "test_data = [\n",
    "    (\n",
    "        [vocabidx_en[token] for token in en_tokens],\n",
    "        en_original,\n",
    "        vi_original\n",
    "    )\n",
    "    for en_tokens, en_original, vi_original in zip(test_en_prep, test_en, test_vi)\n",
    "]\n",
    "\n",
    "vocablist_en, vocabidx_en, vocablist_vi, vocabidx_vi = load_vocab(\n",
    "    VOCAB_DIR, \"vocab_en_new.pkl\", \"vocab_vi_new.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a0e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(FINETUNEMODELNAME, map_location=DEVICE)\n",
    "config = checkpoint.get(\"config\", None)\n",
    "model = Transformer(config[\"src_vocab_size\"], config[\"tgt_vocab_size\"], config[\"d_model\"], config[\"nhead\"],\n",
    "                    config[\"num_encoder_layers\"], config[\"num_decoder_layers\"],\n",
    "                    config[\"dim_feedforward\"], config[\"dropout\"]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b8ab127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Translations ---\n",
      "[1] EN : When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\n",
      "    PRED: Khi I ít gặp nhất là <unk> I được cho là tốt nhất trên toàn thế giới và <unk> I tăng lên một bài báo được gọi là <unk> <unk> <unk>\n",
      "    REF : Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;\n",
      "\n",
      "[2] EN : And I was very proud .\n",
      "    PRED: Và có thể rất dễ bị trầm cảm.\n",
      "    REF : Tôi đã rất tự hào về đất nước tôi .\n",
      "\n",
      "[3] EN : In school , we spent a lot of time studying the history of Kim Il-Sung , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .\n",
      "    PRED: Trong trường học, chúng tôi đã dành nhiều thời gian nghiên cứu tiền sử của <unk> tuy nhiên chúng tôi chưa bao giờ học về bên ngoài thế giới ngoài thế giới và ngoại trừ <unk> <unk> <unk> <unk> là những người có thái độ của <unk>\n",
      "    REF : Ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch Kim II- Sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc Hoa Kỳ , Hàn Quốc và Nhật Bản là kẻ thù của chúng tôi .\n",
      "\n",
      "[4] EN : Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .\n",
      "    PRED: Mặc dù I thường tự do về bên ngoài thế giới và I được cho là người ta dành toàn bộ cuộc sống của người dân Bắc vào Bắc <unk> <unk> cho đến khi mọi người đã thay đổi đột ngột của cuộc sống của người bệnh.\n",
      "    REF : Mặc dù tôi đã từng tự hỏi không biết thế giới bên ngoài kia như thế nào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộc đời ở BắcTriều Tiên , cho tới khi tất cả mọi thứ đột nhiên thay đổi .\n",
      "\n",
      "[5] EN : When I was seven years old , I saw my first public execution , but I thought my life in North Korea was normal .\n",
      "    PRED: Khi được 7 năm tuổi trở về bình thường và có thể thực hiện được thực hiện đầu tiên của chúng tôi là một phương pháp điều trị đầu tiên, nhưng có thể đe doạ tính mạng ở Bắc vào năm Bắc <unk>\n",
      "    REF : Khi tôi lên 7 , tôi chứng kiến cảnh người ta xử bắn công khai lần đầu tiên trong đời , nhưng tôi vẫn nghĩ cuộc sống của mình ở đây là hoàn toàn bình thường .\n",
      "\n",
      "[6] EN : My family was not poor , and myself , I had never experienced hunger .\n",
      "    PRED: Không có gia đình nào bị bệnh và không có triệu chứng nào như là khó chịu và không bao giờ có kinh nghiệm.\n",
      "    REF : Gia đình của tôi không nghèo , và bản thân tôi thì chưa từng phải chịu đói .\n",
      "\n",
      "[7] EN : But one day , in 1995 , my mom brought home a letter from a coworker &apos;s sister .\n",
      "    PRED: Nhưng một ngày sau mổ, trong năm <unk> người ta mang lại một cái đồng nghiệp từ đồng nghiệp và đồng nghiệp của <unk>\n",
      "    REF : Nhưng vào một ngày của năm 1995 , mẹ tôi mang về nhà một lá thư từ một người chị em cùng chỗ làm với mẹ .\n",
      "\n",
      "[8] EN : It read , &quot; When you read this , all five family members will not exist in this world , because we haven &apos;t eaten for the past two weeks .\n",
      "    PRED: Nó được đọc là như vậy Khi bạn đọc được con cái này, tất cả các thành viên trong gia đình sẽ không tồn tại trên thế giới này, vì chúng ta chưa được ăn cho con trong hai tuần qua <unk>\n",
      "    REF : Trong đó có viết : Khi chị đọc được những dòng này thì cả gia đình 5 người của em đã không còn trên cõi đời này nữa , bởi vì cả nhà em đã không có gì để ăn trong hai tuần .\n",
      "\n",
      "[9] EN : We are lying on the floor together , and our bodies are so weak we are ready to die . &quot;\n",
      "    PRED: Chúng tôi nằm trên sàn sàn sàn và các cơ thể của chúng ta sẽ có sẵn sàng để có thể tử vong do bệnh lý này.\n",
      "    REF : Tất cả cùng nằm trên sàn , và cơ thể chúng tôi yếu đến có thể cảm thấy như cái chết đang đến rất gần .\n",
      "\n",
      "[10] EN : I was so shocked .\n",
      "    PRED: I là tình trạng sốc nhiễm khuẩn.\n",
      "    REF : Tôi đã bị sốc .\n",
      "\n",
      "Final BLEU Score on Released Corpus data: 7.32\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model_state'])\n",
    "bleu, _ = evaluate_bleu_greedy(\n",
    "    model, test_data, vocabidx_en, vocabidx_vi,\n",
    "    max_len=60, device=DEVICE, print_samples=True, num_samples=10\n",
    ")\n",
    "print(f\"Final BLEU Score on Released Corpus data: {bleu*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
